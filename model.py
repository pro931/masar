# âœ… Ù…Ø³Ø§Ø± Ø´Ø§Øª Ø¨ÙˆØª - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© ÙˆÙ…Ø±ØªØ¨Ø©
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os

# âœ… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "masar_model11")
DATASET_FAQ = os.path.join(BASE_DIR, "dataset1.csv")
DATASET_TICKETS = os.path.join(BASE_DIR, "dataset.csv")

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR, local_files_only=True)

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
faq_df = pd.read_csv(DATASET_FAQ, quotechar='\"', skipinitialspace=True)
if faq_df.shape[1] == 1:
    faq_df = faq_df[faq_df.columns[0]].str.split(",", n=1, expand=True)
    faq_df.columns = ['question', 'answer']

tickets_df = pd.read_csv(DATASET_TICKETS)

# âœ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
faq_df['question'] = faq_df['question'].str.lower().str.strip()
faq_df['answer'] = faq_df['answer'].str.strip()

# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©
faq_dict = dict(zip(faq_df['question'], faq_df['answer']))

# âœ… Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ØªØ°ÙƒØ±Ø© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„
def check_ticket_and_respond(question):
    for ticket_id in tickets_df['Ticket_ID']:
        if str(ticket_id) in question:
            row = tickets_df[tickets_df['Ticket_ID'] == ticket_id].iloc[0]
            return f"\nğŸ« ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ°ÙƒØ±Ø© Ø±Ù‚Ù… {ticket_id}:\n" \
                   f"ğŸŸï¸ Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©: {row['Match']}\nğŸ“ Ø§Ù„Ù…Ù„Ø¹Ø¨: {row['Stadium']}\nğŸ“¦ Ø§Ù„Ø¨Ù„ÙˆÙƒ: {row['Block']}\n" \
                   f"ğŸ”¢ Ø§Ù„ØµÙ: {row['Row']} | Ø§Ù„ÙƒØ±Ø³ÙŠ: {row['Seat']}\nğŸšª Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©: {row['Gate']}\n" \
                   f"ğŸ…¿ï¸ Ø§Ù„Ù…ÙˆÙ‚Ù: {row['Parking_Zone']}\nğŸ’° Ø§Ù„Ø³Ø¹Ø±: {row['Ticket_Price']} | Ø§Ù„ØªØµÙ†ÙŠÙ: {row['Category']}"
    return None

# âœ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
def generate_response(question):
    question = question.lower().strip()

    if question in faq_dict:
        return faq_dict[question]

    ticket_response = check_ticket_and_respond(question)
    if ticket_response:
        return ticket_response

    inputs = tokenizer(f"question: {question}", return_tensors="pt", truncation=True, max_length=128)
    outputs = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_length=128)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # âœ… Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø· Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ·ÙˆÙŠØ±
    print("ğŸ› ï¸ input:", question)
    print("ğŸ› ï¸ decoded:", decoded)

    return decoded

# âœ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙÙŠØ©
if __name__ == '__main__':
    print("ğŸ¤– Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ! Ø£ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ Ø±Ù‚Ù… ØªØ°ÙƒØ±ØªÙƒ âœ¨")
    while True:
        user_input = input("\nğŸ’¬ Ø£Ù†Øª: ")
        if user_input.strip().lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit']:
            print("ğŸ«¡ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        response = generate_response(user_input)
        print("ğŸ¤– Ù…Ø³Ø§Ø±:", response)
